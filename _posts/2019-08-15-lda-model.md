---
layout: post
title: "Latent-Dirichlet Allocation document categorization         (Python, Natural Langauge Processing)"
date: 2019-08-15
---

Used the Latent-Dirchlet Allocation algorithm in the sklearn Python libray to automatically categorize the top _n_ most popular
news article title subjects over a multiple year period.

# <a href="/assets/lda-model-example.pdf" class="image fit"><img src="assets/lda-model-example.pdf" alt="Project Overview Presentation (PDF)"></a>

<p>Here, I will tell you more about the project...</p>

<p>First, the model generates an unintelligible distribution of topics in the
form of a pie chart:</p>

![pieUnlabeled]({{ site.url }}/assets/pieUnlabeled.png){:height="100%" width="100%"}

<p>Then, we use the way that LDA has
defined each topic via word
associations with that topic to then
define that topic in understandable
terms.
For example, topic 4 shown right: this
might be something like “North Korea
& China”.</p>
![barChart]({{ site.url }}/assets/barTopic4.png){:height="100%" width="100%"}

<p>LAfter looking through each topic
definition, we can re-define our pie
chart with topic definitions that make
sense to us humans.
Now, we’ve transformed 50,120 article
titles into nine topics – in just a few
minutes.</p>
![pieLabeled]({{ site.url }}/assets/pieLabeled.png){:height="100%" width="100%"}

Why is LDA important? Put more broadly, why is natural language processing & machine learning
important?
To find patterns in otherwise un-useable data. Any scenario where there is a mountain of data
that is in the form of language, natural language processing will be useful.
As one thinks about the opportunity & insight this might give their business, it’s important to
also recognize the ethical ramifications of using machine learning techniques on large sets of
data. What if, instead of article titles, this was trans-scripted phone conversation data?